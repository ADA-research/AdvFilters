{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook to get some model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = onnx.load_model(\"./trained_model.onnx\")\n",
    "with open(\"./validation_features.pickle\", \"rb\") as fp:\n",
    "    test_features, test_labels = pickle.load(fp)\n",
    "with open(\"./nsynth_train/class_map.json\", \"r\") as fp:\n",
    "    class_map = json.load(fp)\n",
    "\n",
    "inputs = np.array(test_features, dtype=np.float32)\n",
    "inputs = inputs[:, None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12678, 1, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Accuracy of onnx Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 6786, False predictions: 5892\n",
      "Accuracy: 0.5352579271178419\n"
     ]
    }
   ],
   "source": [
    "providers = ['CPUExecutionProvider']\n",
    "onnx_file_path = \"trained_model.onnx\"\n",
    "output_names = [n.name for n in model.graph.output]\n",
    "m = rt.InferenceSession(onnx_file_path, providers=providers)\n",
    "onnx_pred = m.run(output_names, {\"input\": inputs})[0]\n",
    "\n",
    "correct_preds_onnx = []\n",
    "false_preds_onnx = []\n",
    "for i, preds in enumerate(onnx_pred):\n",
    "    label_pred = np.argmax(preds)\n",
    "    if label_pred == class_map[test_labels[i]]:\n",
    "        correct_preds_onnx.append(i)\n",
    "    else:\n",
    "        false_preds_onnx.append(i)\n",
    "\n",
    "print(f\"Correct predictions: {len(correct_preds_onnx)}, False predictions: {len(false_preds_onnx)}\")\n",
    "accuracy = len(correct_preds_onnx) / (len(correct_preds_onnx) + len(false_preds_onnx))\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 7, 8, 9, 11, 12, 13]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_preds_onnx[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Accuracy of torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/Projects/Verification/venv/lib/python3.11/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:212.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvertModel(\n",
       "  (Transpose_9): Transpose()\n",
       "  (Constant_10): Constant(constant=tensor([ -1, 784]))\n",
       "  (Reshape_11): Reshape(shape=None)\n",
       "  (Gemm_12): Linear(in_features=784, out_features=1024, bias=True)\n",
       "  (Relu_13): ReLU(inplace=True)\n",
       "  (Gemm_14): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (Relu_15): ReLU(inplace=True)\n",
       "  (Gemm_16): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (Relu_17): ReLU(inplace=True)\n",
       "  (Gemm_18): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from onnx2pytorch import ConvertModel\n",
    "onnx_model_untrained = onnx.load_model(\"./nsynth_train/mnist_relu_4_1024.onnx\")\n",
    "torch_model = ConvertModel(onnx_model_untrained)\n",
    "torch_model.load_state_dict(torch.load(\"./model_20240307_165159_0\"))\n",
    "torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 6786, False predictions: 5892\n",
      "Accuracy: 0.5352579271178419\n"
     ]
    }
   ],
   "source": [
    "from nsynth_train.train_model import NSynthDataset\n",
    "test_dataset = NSynthDataset(picklefile=\"./validation_features.pickle\", class_map=\"./nsynth_train/class_map.json\")\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "torch_model.eval()\n",
    "correct_preds = []\n",
    "false_preds = []\n",
    "with torch.no_grad():\n",
    "    for i, vdata in enumerate(test_dataset_loader):\n",
    "        vinputs, vlabels = vdata['melfeatures'], vdata['instrument']\n",
    "        vinputs = vinputs.to(torch.float)\n",
    "        vinputs = vinputs[None, :, :, :]\n",
    "        voutputs = torch_model(vinputs)\n",
    "        class_pred = np.argmax(voutputs)\n",
    "        if class_pred == vlabels:\n",
    "            correct_preds.append(i)\n",
    "        else:\n",
    "            false_preds.append(i)\n",
    "\n",
    "print(f\"Correct predictions: {len(correct_preds)}, False predictions: {len(false_preds)}\")\n",
    "accuracy = len(correct_preds) / (len(correct_preds) + len(false_preds))\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 7, 8, 9, 11, 12, 13]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_preds[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
